{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KWf9aWoHk-rT"
   },
   "source": [
    "# Welcome to the PyTorch Sentiment Analysis Exercise\n",
    "\n",
    "This exercise will cover some key concepts in natural language machine learning, including:\n",
    "\n",
    "* The preparation of text datasets\n",
    "* The construction of neural net models for natural language tasks\n",
    "* The use of *recurrent neural network layers,* which are commonly used in problems involving sequences, including natural language tasks.\n",
    "\n",
    "## Notes on Using This Notebook\n",
    "\n",
    "* Code will be provided for boilerplate tasks; in other places, you will need to fill in code to complete the exercise. Cells you need to fill in will be flagged with the **Exercise** heading.\n",
    "* The code cells are, in general, meant to be run in order. If you think a code cell should be working, but it isn't, verify that all previous cells were run - the cell you're having trouble with may depend on a variable or file that is created in a previous cell.\n",
    "* Class names and other text normally meant for consumption by a computer will be rendered in a `monospace font`. This will hopefully reduce confusion between, e.g., the word \"dataset\" referring to the concept of a cohesive body of data, and the class name `Dataset` referring to the related PyTorch class.\n",
    "\n",
    "### Do This Now:\n",
    "\n",
    "The cell below downloads and unzips the dataset we'll be using for this exercise. The dataset isn't huge, but it may take a minute to download, so **please uncomment and execute the following code cell now** to get the process started. (The commented lines are there to prevent the download triggering accidentally, so you may wish to replace them afterward.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6p2iEyrC34qE"
   },
   "outputs": [],
   "source": [
    "# !curl -0 https://s3-us-west-1.amazonaws.com/pytorch-course-datasets/sentiment-analysis-on-movie-reviews.zip > reviews.zip\n",
    "# !unzip reviews.zip\n",
    "# !mkdir models\n",
    "# !pip install torchtext\n",
    "# !pip install spacy\n",
    "# !python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k83QPxu2lEJ1"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This exercise is based on the Kaggle competition, [Sentiment Analysis on Movie Reviews](https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/overview). The goal is to accurately classify movie reviews based on how positively they speak of the movie being reviewed.\n",
    "\n",
    "### The Training Dataset\n",
    "\n",
    "Let's have a look at the input data. In `train.tsv`, you should see a file with four fields: A phrase ID, a sentence ID, a phrase, and a sentiment score. (The score runs from 0 to 4, with higher numbers indicating more positive sentiment.) You should see the same content in `test.tsv`, but without the scores.\n",
    "\n",
    "### The Approach\n",
    "\n",
    "There are many approaches to Natural Language Processing (NLP) using deep learning, and they will often include a variety of common neural network constructs, including fully-connected layers, recurrent neural networks (RNNs), convnets, embeddings, and dropouts. For this exercise, we'll be guiding you through constructing a simple model using word embeddings and RNNs, and pointing out directions for further enhancement and exploration.\n",
    "\n",
    "### The Final Step\n",
    "\n",
    "The test dataset is a separate, unlabeled collection of phrases from movie reviews. The final step in today's exercise will be to use your model to classify the unlabeled phrases. You will export your predictions to a file and upload them to the Kaggle site to receive a final accuracy score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XheP7WvtlK2x"
   },
   "source": [
    "## Getting Started\n",
    "\n",
    "In the cells below, we will:\n",
    "* ...import some necessary modules\n",
    "* ...define a few boilerplate helper functions\n",
    "* ...create a global device handle for our CPU or GPU device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qenT9rGd3_Tl"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext import data       # NLP data utilities\n",
    "\n",
    "import random\n",
    "import time                      # timestamping messages and saved models\n",
    "import os                        # for filesystem methods\n",
    "\n",
    "\n",
    "# some constants\n",
    "BATCH_SIZE=64\n",
    "EPOCHS=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w7zmDkFp4O5W"
   },
   "outputs": [],
   "source": [
    "def tlog(msg):\n",
    "    print('{}   {}'.format(time.asctime(), msg))\n",
    "\n",
    "def count_model_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def count_correct(guesses, labels):\n",
    "    return (guesses == labels).float().sum()\n",
    "\n",
    "def save_model(model, epoch):\n",
    "    savefile = \"{}-e{}-{}.pt\".format('pytorch-sentiment', epoch, int(time.time()))\n",
    "    tlog('Saving model {}'.format(savefile))\n",
    "    path = os.path.join('models', savefile)\n",
    "    # recommended way from https://pytorch.org/docs/stable/notes/serialization.html\n",
    "    torch.save(model.state_dict(), path)\n",
    "    return savefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lq5x7Ri44llU"
   },
   "outputs": [],
   "source": [
    "# global device handle\n",
    "if not torch.cuda.is_available():\n",
    "    device = torch.device('cpu')\n",
    "    print('*** GPU not available - running on CPU. ***')\n",
    "else:\n",
    "    device = torch.device('cuda')\n",
    "    print('GPU ready to go!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_xTM35uul_D9"
   },
   "source": [
    "## Setting Up the Training Data\n",
    "\n",
    "Setting up natural language datasets in PyTorch is a bit different than setting up numeric time series or image data. For this reason, the PyTorch domain library `torchtext` includes a number of utilities that help with handling data for NLP tasks.\n",
    "\n",
    "### `Field` Objects\n",
    "\n",
    "NLP datasets are often presented in a structured form, such as a CSV or TSV file, where individual fields can be identified. To ease consumption of this data, `torchtext` offers a `Field` class, and related subclasses, as well as a `TabularDataset` class. The `Field` classes let you define datatypes and transformations (such as tokenization) on a field, and the `TabularDataset` takes a list of fields together with information about the file and its format, to parse the file into an internal representation that we'll find convenient. (See lines 10-26 in the code cell below.)\n",
    "\n",
    "For this iteration of the exercise, we will skip the first two fields in the dataset, and create a single field for each phrase to be analyzed, and a field for the sentiment score associated with that field.\n",
    "\n",
    "### Vocabularies and Embedding\n",
    "\n",
    "The standard way of representing words in a natural language vocabulary is with **\"one-hot\" vectors** (known as *unit vectors* in other mathematical contexts). For a three-word vocabulary, this would look like:\n",
    "\n",
    "```\n",
    "word0 = torch.tensor([1, 0, 0])\n",
    "word1 = torch.tensor([0, 1, 0])\n",
    "word2 = torch.tensor([0, 0, 1])\n",
    "```\n",
    "\n",
    "Native speakers of most languages know 15,000 to 20,000 words; you can see how the above representation would consume untenable amounts of memory for large datasets. For this reason, PyTorch represents one-hot vectors by storing a single integer, which is the *index* of the single \"hot\" entry of the word vector.\n",
    "\n",
    "Even with this optimization, the one-hot representation doesn't capture interesting features of the vocabulary, such as the semantic closeness of word pairs like \"fantastic\" and \"amazing\". The vocabulary vector space can be reduced further by *embedding* the vocabulary in a lower-dimensional space, which has the effect of reducing the complexity of the space in which we're trying to find ad descend gradients while capturing some of these relationships within the vocabulary.\n",
    "\n",
    "You can create and train your own embedding using the `torch.nn.Embedding` layer, or you can use one of the pre-trained embeddings made available through `torchtext`. Below, we make use of the GloVe word embedding when we build our vocabulary for the `phrase` field in line 29."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mBTlMfsM4bBF"
   },
   "outputs": [],
   "source": [
    "# dataset constants\n",
    "VOCAB_SIZE = 15000 # max size of vocabulary\n",
    "VOCAB_VECTORS = \"glove.6B.100d\" # Stanford NLP GloVe (global vectors) for word rep\n",
    "\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    tlog('Preparing data...')\n",
    "    phrases_fieldspec = data.Field(include_lengths=True, tokenize='spacy')\n",
    "    labels_fieldspec = data.LabelField(dtype=torch.int64, sequential=False)\n",
    "\n",
    "    fields = [\n",
    "        ('SKIP_phrase_id', None),\n",
    "        ('SKIP_sentence_id', None),\n",
    "        ('phrases', phrases_fieldspec),\n",
    "        ('labels', labels_fieldspec)\n",
    "    ]\n",
    "\n",
    "\n",
    "    train_data = data.TabularDataset(\n",
    "        'train.tsv', # path to file\n",
    "        'TSV', # file format\n",
    "        fields,\n",
    "        skip_header = True # we have a header row\n",
    "    )\n",
    "    \n",
    "    train_data, eval_data = train_data.split()\n",
    "    phrases_fieldspec.build_vocab(train_data, max_size=VOCAB_SIZE, vectors=VOCAB_VECTORS)\n",
    "    labels_fieldspec.build_vocab(train_data)\n",
    "    vocab_size = len(phrases_fieldspec.vocab)\n",
    "    output_size = len(labels_fieldspec.vocab)\n",
    "    \n",
    "    train_iter, eval_iter = data.BucketIterator.splits(\n",
    "        (train_data, eval_data), \n",
    "        batch_size=BATCH_SIZE,\n",
    "        device=device, sort=False, shuffle=True)\n",
    "    \n",
    "    tlog('Data prepared')\n",
    "    return train_iter, eval_iter, vocab_size, output_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3yMDM2EitxCx"
   },
   "source": [
    "## The Model\n",
    "\n",
    "The model below usually gets to about 60% or better accuracy within 30 epochs on this particular sentiment analysis problem. Below, we describe a few key features of the model.\n",
    "\n",
    "### Embedding\n",
    "\n",
    "The first layer of the model is the embedding layer. We described vocabulary embeddings above; here, we show how to take the weights of the pretrained embedding and use them within your own model.\n",
    "\n",
    "Note that we have set `requires_grad` on the embedding layer's weights to `False` (line 17 below). This freezes the embedding, so that it does not participate in the learning process. Leaving `requires_grad` as `True` for this layer's weights would mean radically increasing the number of learning parameters in the model.\n",
    "\n",
    "### Recurrent Neural Networks\n",
    "\n",
    "A *recurrent neural network* RNN may refer to one of two things: The first is a type of neural network layer that maintains a \"hidden\" state, separate from its learning weights, that lets it carry forward state when it deals with sequential data. The second is a class of neural network layers that includes the basic RNN and more complex constructs employing similar techniques, which include the *gated recurrent unit* (GRU) and *long short-term memory* (LSTM).\n",
    "\n",
    "For this exercise, we will employ an LSTM. It adds architectural complexity to the model, but has the advantage that it is much less susceptible to the problems of *vanishing gradients* and *exploding gradients* that are often seen when training RNNs on long sequences. We'll use two LSTM layers to start (lines 19-23). One important thing to note about the LSTM is that instead of a single hidden state vector, it maintains a separate *memory cell* with separate mechanisms for determining when to \"remember\" or \"forget\" certain computation paths.\n",
    "\n",
    "### Classification\n",
    "\n",
    "We'll model our possible integer outputs from 0 to 4 as five possible output classes, and use a linear layer at the end of the model to perform the final classification (line 25).\n",
    "\n",
    "### The Computation Graph\n",
    "\n",
    "In the `forward()` method, there are a couple of interesting features to how the model layers are composed.\n",
    "\n",
    "First, note that the `hidden` state is passed out of the model when the computation returns, and is *passed back in* by the caller on subsequent calls. For the case in which the input hidden state is `None`, the model initializes it correctly.\n",
    "\n",
    "Second, the `hidden` state consumed and returned by `self.lstm`  is actually a tuple containing the hidden state vector and the memory cell. When we pass the output of the LSTM to the classifier, we specify it as `hidden[0][-1]`. The `[0]` specifies the first element of the (hidden, cell) tuple, and the [-1] specifies the hidden state returned by the final LSTM layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GFJ3dT4g40-7"
   },
   "outputs": [],
   "source": [
    "# model constants\n",
    "EMBEDDING_SIZE = 100 # must match dimensions in vocab vectors above\n",
    "HIDDEN_SIZE = 100\n",
    "OUTPUT_SIZE = 5 # 0 to 4\n",
    "NUM_LAYERS = 2\n",
    "\n",
    "\n",
    "\n",
    "class SentimentAnalyzer(nn.Module):\n",
    "    def __init__(self, embedding_size, pretrained_embedding, hidden_size, output_size, batch_size):\n",
    "        super(SentimentAnalyzer, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_embedding, freeze=True)\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_size,\n",
    "            num_layers=NUM_LAYERS,\n",
    "            hidden_size=hidden_size\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, phrases, hidden):\n",
    "        x = self.embedding(phrases)\n",
    "        x, hidden = self.lstm(x, hidden)\n",
    "        x = self.fc(hidden[0][-1]) # remove [0] for RNN or GRU, [-1] is for layers\n",
    "        return x.squeeze(0), hidden\n",
    "    \n",
    "    def init_hidden(self, device):\n",
    "        return (torch.zeros(NUM_LAYERS, self.batch_size, self.hidden_size, dtype=torch.float).to(device),\n",
    "                torch.zeros(NUM_LAYERS, self.batch_size, self.hidden_size, dtype=torch.float).to(device))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0pjcUBWt029o"
   },
   "source": [
    "## Training the Model\n",
    "\n",
    "The following code cells define some functions to support our model training process, the structure of which should be familiar by now. Go ahead and run them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PiUg5J_Q6m1w"
   },
   "outputs": [],
   "source": [
    "def get_model(output_size, vectors):\n",
    "    tlog('Creating model...')\n",
    "    sa = SentimentAnalyzer(EMBEDDING_SIZE, vectors, HIDDEN_SIZE, output_size, BATCH_SIZE)\n",
    "    tlog('The model has {} trainable parameters'.format(count_model_params(sa)))\n",
    "    tlog(sa)\n",
    "    return sa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SaIJfIbk65HC"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, loss_fn, optimizer): # one epoch\n",
    "    curr_loss = 0.\n",
    "    curr_correct = 0.\n",
    "    model.train() # makes sure that training-only fns, like dropout, are active\n",
    "    \n",
    "    for batch in iterator:\n",
    "        hidden = model.init_hidden(device)\n",
    "        # get the data\n",
    "        phrases, lengths = batch.phrases\n",
    "        \n",
    "        if phrases.shape[1] == BATCH_SIZE:        \n",
    "            # predict and learn\n",
    "            optimizer.zero_grad()\n",
    "            guesses, hidden = model(phrases, hidden)\n",
    "            loss = loss_fn(guesses, batch.labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            hidden[0].detach_() # or we get double-backward errors\n",
    "            hidden[1].detach_() # or we get double-backward errors\n",
    "\n",
    "            # measure\n",
    "            curr_loss += loss.item()\n",
    "            curr_correct += count_correct(torch.argmax(guesses, 1), batch.labels)\n",
    "        \n",
    "    return curr_loss / len(iterator), curr_correct / (len(iterator) * BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8wU0mXmR7CL1"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()  # not training\n",
    "def evaluate(model, iterator, loss_fn):\n",
    "    curr_loss = 0.\n",
    "    curr_correct = 0.\n",
    "    model.eval() # makes sure that training-only fns, like dropout, are inactive\n",
    "    \n",
    "    for batch in iterator:\n",
    "        hidden = model.init_hidden(device)\n",
    "        # get the data\n",
    "        phrases, lengths = batch.phrases\n",
    "\n",
    "        if phrases.shape[1] == BATCH_SIZE:        \n",
    "            # predict\n",
    "            guesses, hidden = model(phrases, hidden) # .squeeze(1)\n",
    "            loss = loss_fn(guesses, batch.labels)\n",
    "\n",
    "            # measure\n",
    "            curr_loss += loss.item()\n",
    "            curr_correct += count_correct(torch.argmax(guesses, 1), batch.labels)\n",
    "\n",
    "    \n",
    "    return curr_loss / len(iterator), curr_correct / (len(iterator) * BATCH_SIZE)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eMySlJKl1Cmg"
   },
   "source": [
    "Below, we define the training loop. The basic structures of bookkeeping, training, evaluation, reporting, and saving the model should be familiar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vDmduE-x7Cuc"
   },
   "outputs": [],
   "source": [
    "# training loop constants\n",
    "EPOCHS = 30\n",
    "LR = 1e-3\n",
    "\n",
    "\n",
    "def learn(model, train_iter, eval_iter):\n",
    "    eval_losses = []\n",
    "    eval_accs = []\n",
    "    best_eval_acc = 0\n",
    "    \n",
    "    model = model.to(device)\n",
    "\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.Adam(params, lr=LR)\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        tlog('EPOCH {} of {}'.format(epoch + 1, EPOCHS))\n",
    "        \n",
    "        train_loss, train_acc = train(model, train_iter, loss_fn, optimizer)\n",
    "        tlog('  Training loss {}   acc {}'.format(train_loss, train_acc))\n",
    "        \n",
    "        eval_loss, eval_acc = evaluate(model, eval_iter, loss_fn)\n",
    "        tlog('  Validation loss {}   acc {}'.format(eval_loss, eval_acc))\n",
    "        eval_losses.append(eval_loss)\n",
    "        eval_accs.append(eval_acc)\n",
    "        if eval_acc > best_eval_acc:\n",
    "            tlog('  *** New accuracy peak, saving model')\n",
    "            best_eval_acc = eval_acc\n",
    "            saved_model_filename = save_model(model, epoch + 1)\n",
    "    \n",
    "    tlog('DONE')\n",
    "    tlog('Best accuracy: {}'.format(best_eval_acc))\n",
    "    return eval_losses, eval_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pzlnsv0n1egp"
   },
   "source": [
    "Let's create our dataset and build the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nql6BMtcWQIm"
   },
   "outputs": [],
   "source": [
    "t_iter, e_iter, vocab_size, output_size = get_data()\n",
    "embedding_vectors = t_iter.dataset.fields['phrases'].vocab.vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_G54ySpm1i1g"
   },
   "source": [
    "And finally, let's create the model and train it. You should see the loss decreasing and the accuracy increasing (more-or-less) over the training epochs, up to about 60% or better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FZLEmqGx7G_z"
   },
   "outputs": [],
   "source": [
    "sa = get_model(output_size, embedding_vectors)\n",
    "losses, accs = learn(sa, t_iter, e_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VZl4QBcQ_MQQ"
   },
   "source": [
    "## Exercise: Refining the Model\n",
    "\n",
    "It is not expected that you'll make it through every one of the suggestions below in the half-day allotted to complete this class unit. The intent is for you to read through, pick avenues of exploration that sound promising to you, and experiment with them.\n",
    "\n",
    "### Training\n",
    "\n",
    "Do you feel like your model is converging, or might it need more epochs? Would changing the optimization algorithm, or the learning rate, or specifying other hyperparameters (such as the beta coefficients used for the `Adam` optimizer) improve the outcome?\n",
    "\n",
    "Do the loss and accuracy values of your training and validation steps track closely, or do the training scores continue improving while the validation scores plateau? (See the section below, **Dropout,** for notes on overfitting.)\n",
    "\n",
    "### Dropout\n",
    "\n",
    "Deep learning models are sometimes susceptible to *overfitting,* which is what happens when a model \"memorizes\" a relatively small training set and performs well on it, but does poorly when presented with new data. One method for preventing this is called *dropout,* which is the practice of randomly \"dropping out\" nodes of your neural network and temporarily setting their weights to zero.\n",
    "\n",
    "This method is so commonly used that PyTorch specifies a separate argument for it for use with multi-layer RNNs. What happens if you change the creation of the model's LSTM layers to read:\n",
    "\n",
    "```\n",
    "self.lstm = nn.LSTM(\n",
    "    input_size=embedding_size,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    hidden_size=hidden_size,\n",
    "    dropout=0.5 # <--- new arg\n",
    ")\n",
    "```\n",
    "\n",
    "A dropout can also be added as a function or a layer in your model. For example, in the `__init__()` method for your model, you can add:\n",
    "\n",
    "```\n",
    "self.dropout = nn.Dropout(0.5)\n",
    "```\n",
    "\n",
    "Then, in `forward()`, you can make changes like:\n",
    "\n",
    "```\n",
    "x = self.dropout(self.embedding(phrases))\n",
    "```\n",
    "\n",
    "**Consider exploring the use of dropout in your model to see if you can improve accuracy.** What layers should you apply it to? Is the default value of 0.5 optimal, or is there benefit to tuning it?\n",
    "\n",
    "### Bidirectional LSTM\n",
    "\n",
    "A *bidirectional* LSTM weaves together two layers of LSTM nodes, one processing the word sequence in the forward direction, the other processing it backward. Why would we do this? Context is important in NLP, but if we only process from beginning to end, we only have access to the context to one side of the word currently being examined. For example, in a simple sentence like \"Alice teamed up with Bob\", the names of the individuals may be regarded as equally important pieces of context to understanding the situation the verb describes. There are various strategies for building local context when interpreting a sentence, such as breaking the sentence into n-grams, or using convnets. One method often used with RNNs is to simultaneously run the input through two RNN layers, one backward and one forward, and concatenate their output for each word, which captures context to either side of the word.\n",
    "\n",
    "PyTorch makes this advanced method easier by letting you specify 'bidirectional=True' when creating any of the RNN layer types (RNN, GRU, or LSTM). Your LSTM already has two layers, and could take this flag with no other changes to the LSTM layer.\n",
    "\n",
    "**Consider experimenting with a bidirectional LSTM.** Are there other architectural changes you need to make to your model for this to work?\n",
    "\n",
    "### Classification vs. Scalar Score\n",
    "\n",
    "In the model defined above, we chose to view the sentiment score from 0 to 4 as *5 classes of output,* rather than as a scalar score.\n",
    "\n",
    "**Consider changes to classification.** Currently, we have only one fully-connected layer to perform our classification, but some well-known models - even small but effective ones like LeNet-5 - use two or more FC layers for classification. What's the benefit to adding one or more layers (with an appropriate activation function between them)?\n",
    "\n",
    "**Consider strategies besides classification.** What if we treated the sentiment score as a floating-point scalar, bounded between 0 and 4, and asked the model to output a single score as output? You might consider changes like the following:\n",
    "\n",
    "* The dataset labels would need an extra step to convert them to floating point numbers - look in the `torchtext` docs for the `postprocessing` argument to the `Field` constructor.\n",
    "* It might be beneficial to change the numeric sentiment score to lie in the range [-1, 1] - besides being an intuitive mapping to positive/negative sentiment, it also places the middle of the scale at 0, which is where many activation functions have their steepest gradients.\n",
    "* Making this change will also involve changing the final layer to produce a single output instead of 5. For the sake of stability, it might be wise to apply a function like `tanh` or `hardtanh` to clamp the output to the intended range.\n",
    "* To produce output for the Kaggle site to consume, you'll also need to map the [-1, 1] range back to [0,4], and round to the nearest legal integer value.\n",
    "\n",
    "### Vocabularies and Fine-Tuning\n",
    "\n",
    "In specifying our model, we used a pre-trained embedding made available through PyTorch. Others are available, or you could train your own. **Under what conditions might you choose another model, or train your own?**\n",
    "\n",
    "We also deliberately froze the learning weights of the embedding. In previous lessons, we discussed fine-tuning pre-trained convnets against new output classes. **Is there any advantage to allowing the pre-trained embedding layer to take part in learning?**\n",
    "\n",
    "### Other Model Types and Arrangements of Data\n",
    "\n",
    "There are multiple sentiment analysis tutorials on the web, and some of them make use of non-RNN or hybrid techniques, including the use of convnets. The basic idea of using convolutions against text data leans on the multi-dimensional nature of word embeddings. For example, imagine that we've embedded our vocabulary in a 5-dimensional space - i.e., every word is represented by a vector of 5 floating-point numbers. A sentence of *N* words is then an *N x 5* tensor, and we've seen from previous lessons how to apply a 2D convnet to such a tensor. The difference here is that the convolution kernel will always be *N* elements wide, but may be 3 or 5 (or more) elements tall, so as to capture context around the word currently being examined.\n",
    "\n",
    "**Consider applying convnet methods to the sentiment analysis problem.** Would you want to only include CNNs, or would you include RNN methods as well? How do you deal with varying sentence length? Would you want to pad the input so that the convolution kernel stops at every word?\n",
    "\n",
    "There are other methods for incorporating a word's local context. For example, you could stick with RNN methods, but break your sentence into n-grams instead of single words. For bigrams, a sentence like \"I really enjoyed this movie.\" might look like:\n",
    "\n",
    "```\n",
    "[('I', 'really'), ('really', 'enjoyed'), ('enjoyed', 'this'), ('this', 'movie')]\n",
    "```\n",
    "\n",
    "**Consider how you might massage your data to achieve better results.** If you employ n-grams, how long should they be? How would you adjust your model architecture to incorporate the differently-shaped data?\n",
    "\n",
    "### Including More Data\n",
    "\n",
    "In specifying our dataset, we deliberately ignored the `PhraseId` and `SentenceId` fields of the training data. **Can you picture a way that including either of these fields might help?** How would you include a new field in your data representation? How would you accomodate it in your model architecture?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "pytorch_estimator = PyTorch('sentiment_analysis.py',\n",
    "                            source_dir='sentiment-estimator',\n",
    "                            train_instance_type='ml.p2.xlarge', # 'ml.p3.2xlarge',\n",
    "                            train_instance_count=1,\n",
    "                            framework_version='1.0.0',\n",
    "                            role='arn:aws:iam::896498678582:role/service-role/AmazonSageMaker-ExecutionRole-20190226T104608',\n",
    "                            hyperparameters = {'epochs': 20, 'batch-size': 64, 'learning-rate': 0.001})\n",
    "\n",
    "pytorch_estimator.fit('s3://pytorch-course-datasets-uswest2/sentiment-train/')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "PyTorch Sentiment Analysis Exercise.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
